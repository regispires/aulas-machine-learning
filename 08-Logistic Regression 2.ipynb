{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After step 1, likelihood: [[-6931.4718056]]; accuracy: 0.5\n",
      "After step 5001, likelihood: [[-309.43671144]]; accuracy: 0.9892\n",
      "After step 10001, likelihood: [[-308.96007441]]; accuracy: 0.9893\n",
      "After step 15001, likelihood: [[-308.94742145]]; accuracy: 0.9893\n",
      "After step 20001, likelihood: [[-308.94702925]]; accuracy: 0.9893\n",
      "After step 25001, likelihood: [[-308.94702533]]; accuracy: 0.9893\n",
      "After step 30001, likelihood: [[-308.94702849]]; accuracy: 0.9893\n",
      "After step 35001, likelihood: [[-308.94701465]]; accuracy: 0.9893\n",
      "After step 40001, likelihood: [[-308.94701912]]; accuracy: 0.9893\n",
      "After step 45001, likelihood: [[-308.94702355]]; accuracy: 0.9893\n",
      "[[-10.20181874]\n",
      " [ -2.64493647]\n",
      " [  5.4294686 ]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(3)\n",
    "num_pos = 5000\n",
    "learning_rate = 0.0001\n",
    "epochs = 50000\n",
    "\n",
    "# Bivariate normal distribution mean [0, 0] [0.5, 4], with a covariance matrix\n",
    "subset1 = np.random.multivariate_normal([0, 0], [[1, 0.6],[0.6, 1]], num_pos)\n",
    "subset2 = np.random.multivariate_normal([0.5, 4], [[1, 0.6],[0.6, 1]], num_pos)\n",
    "\n",
    "dataset = np.vstack((subset1, subset2))\n",
    "x = np.hstack((np.ones(num_pos*2).reshape(num_pos*2, 1), dataset)) # add 1 for beta_0 intercept\n",
    "label = np.hstack((np.zeros(num_pos), np.ones(num_pos)))\n",
    "y = label.reshape(num_pos*2, 1) # reshape y to make 2D shape (n, 1)\n",
    "beta = np.zeros(x.shape[1]).reshape(x.shape[1], 1)\n",
    "\n",
    "for step in np.arange(epochs):\n",
    "    x_beta = np.dot(x, beta)\n",
    "    y_hat = 1 / (1 + np.exp(-x_beta))\n",
    "    likelihood = np.sum(np.log(1 - y_hat)) + np.dot(y.T, x_beta)\n",
    "    preds = np.round( y_hat )\n",
    "    accuracy = np.sum(preds == y)*1.00/len(preds)\n",
    "    gradient = np.dot(np.transpose(x), y - y_hat)\n",
    "    beta = beta + learning_rate*gradient\n",
    "    if( step % 5000 == 0):\n",
    "        print(\"After step {}, likelihood: {}; accuracy: {}\".format(step+1, likelihood, accuracy))\n",
    "    \n",
    "\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.17937262] [[-2.63894088  5.41747923]]\n"
     ]
    }
   ],
   "source": [
    "# comparar com o resultado do sklearn\n",
    "from sklearn import linear_model\n",
    "# Logistic regression class in sklearn comes with L1 and L2 regularization, \n",
    "# C is 1/lambda; setting large C to make the lamda extremely small \n",
    "clf = linear_model.LogisticRegression(C = 100000000, penalty=\"l2\")\n",
    "clf.fit(dataset, label)\n",
    "print(clf.intercept_, clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
